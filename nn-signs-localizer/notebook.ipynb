{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorium Automatyka Pojazdowa: system wizyjny rozpoznający znaki drogowe\n",
    "\n",
    "## Część 2: Lokalizacja\n",
    "\n",
    "Problem do rozwiązania w ramach tego laboratorium jest zadaniem lokalizacji: określenia regionów, w których znajdują się na obrazie obiekty - w naszym przypadku, znaki drogowe, ale jeszcze bez wskazania klasy.\n",
    "W rzeczywistości wskazany problem jest skomplikowany, zatem wykorzystany zostanie transfer learning - technika polegająca na wykorzystaniu wytrenowanego na dużo bardziej zaawansowanym, szerszym, datasecie, modelu ekstraktora cech (sieci konwolucyjnej) i trenowanie jedynie wyjściowej warstwy gęsto połączonej (tzw. backbone-a).\n",
    "\n",
    "Wykorzystane zostanie Tensorflow Object Detection API oraz [model RetinaNet](https://paperswithcode.com/method/retinanet) pretrenowany na datasecie [MS COCO (Common Objects in COntext)](https://cocodataset.org/), który stanowi bardzo dobry detektor (czyli model przeprowadzający lokalizację oraz klasyfikację jednocześnie) obiektów.\n",
    "Wykorzystywany dataset posiada jedynie informacje o bounding boxach oznaczających znaki w katalogu [data/detection/labels](data/detection/labels) (wystarczy spojrzeć na dowolny plik TXT z labelem, np. [`1487.txt`](../data/detection/labels/1487.txt) oraz dla \"orientacji\" [`1487.jpg`](../data/detection/imgs/1487.jpg)) - w każdym pliku każda linia jest postaci `<klasa> <xCenter %> <yCenter %> <width %> <height %>`, jednakże `klasa` zawsze jest 0 - dataset nie zawiera więcej informacji. Z uwagi na to, w tym ćwiczeniu nie tworzymy w praktyce detektora, a jedynie lokalizator.\n",
    "\n",
    "Miejsca w kodzie pozostawione do uzupełnienia zostały w tym notatniku oznaczone `# TODO`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import *\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import tensorflow_models as tfm\n",
    "\n",
    "from official.core import exp_factory\n",
    "from official.core import config_definitions as cfg\n",
    "from official.vision.serving import export_saved_model_lib\n",
    "from official.vision.ops.preprocess_ops import normalize_image\n",
    "from official.vision.ops.preprocess_ops import resize_and_crop_image\n",
    "from official.vision.utils.object_detection import visualization_utils\n",
    "from official.vision.dataloaders.tf_example_decoder import TfExampleDecoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Konwersja datasetu do formatu TFrecord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TF Models API - TFM]() - operuje na datasetach w postaci TFrecord. Jest to format skompresowanych i zoptymalizowanych zbiorów danych dla tensorflow, zapisywany do jednego pliku.\n",
    "W tej sekcji zadaniem dla Państwa jest:\n",
    "- zaczytanie zdjęć oraz plików TXT w formacie opisanym w sekcji powyżej w celu utworzenia zbioru danych (`tf.data.Dataset`) z obrazami oraz oznaczonymi na nich bounding boxami jedynej klasy - znaku drogowego ('jakiegokolwiek')\n",
    "- podziału datasetu na zbiór treningowy i ewaluacyjny w stosunku $80 : 20$\n",
    "- zapisania obydwóch datasetów do plików: `signs-train.tfrecord` oraz `signs-eval.tfrecord`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wczytywanie danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W poniższej komórce znajdują się parametry odpowiedzialne za konstrukcję datasetu, dla których - po wypełnieniu pozostałych komórek - kolejno należy uruchamiać notatnik i zanotować wyniki: `BATCH_SIZE` - należy sprawdzić kolejno działanie dla wartości 4, 16, 32. Batch size - rozmiar wsadu - to parametr sterujący ilością obrazków w pojedynczym wsadzie, dla którego są kolejno wykonywane: forward propagation, backward propagation, optymalizacja wag sieci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = ... # TODO: należy dobrać sensowny rozmiar wsadu\n",
    "IMG_W = ... # TODO: ustawić na 512px\n",
    "IMG_H = ... # TODO: ustawić na 256px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W poniższej komórce znajdują się implementacje metod odpowiedzialnych za ładowanie danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def readImage(imagePath: str):\n",
    "    \"\"\"Funkcja ładująca obrazek ze ścieżki imagePath\"\"\"\n",
    "    rawBytes = tf.io.read_file(imagePath)\n",
    "    \n",
    "    return tf.io.decode_jpeg(rawBytes)\n",
    "\n",
    "def read_image_size_for_tf(image: tf.Tensor) -> List[int]:\n",
    "    # TODO: image jest tensorem tf.Tensor; aby otrzymać np.ndarray będący obrazem RGB o kształcie [Height, Width, Depth], należy wykonać image.numpy()\n",
    "    # poniższy kod ma zwracać Listę 2 integerów w postaci [Width, Height]\n",
    "    ...\n",
    "    return [W, H]\n",
    "\n",
    "# TODO: poniżej należy wskazać relatywną ścieżkę do katalogu detection/imgs w datasecie\n",
    "images = tf.data.Dataset.list_files('../<fragment sciezki - TODO>/*.jpg', shuffle=False)\n",
    "images = images.map(readImage)\n",
    "imageSizesOriginal = images.map(lambda x: tf.py_function(read_image_size_for_tf, [x], [tf.uint64, tf.uint64]))\n",
    "images = images.map(lambda x: tf.image.resize(x, (IMG_H, IMG_W)))\n",
    "# TODO: znormalizuj wartości pikseli (z obecnych elementów uint8 0-255 na float32 0-1)\n",
    "images = images.map(lambda image: ...)\n",
    "\n",
    "def load_labels(label_path):\n",
    "    BBs = []\n",
    "    label_path = label_path.numpy()\n",
    "\n",
    "    with open(label_path) as f:\n",
    "        for line in filter(lambda l: len(l.strip()), f.readlines()):\n",
    "            _, xCenter, yCenter, width, height = map(float, line.split())\n",
    "\n",
    "            # TODO: wylicz współrzędne górnego lewego punktu bounding boxa (xMin, yMin) oraz prawego dolnego (xMax, yMax)\n",
    "            halfSignW = ...\n",
    "            halfSignH = ...\n",
    "            xMin = ...\n",
    "            yMin = ...\n",
    "            xMax = ...\n",
    "            yMax = ...\n",
    "\n",
    "            BBs.append((yMin, xMin, yMax, xMax))\n",
    "\n",
    "    filename = os.path.basename(label_path)\n",
    "    return filename, BBs\n",
    "\n",
    "# TODO: poniżej należy wskazać relatywną ścieżkę do katalogu detection/labels w datasecie\n",
    "labelPaths = tf.data.Dataset.list_files('../<fragment sciezki - TODO>/*.txt', shuffle=False)\n",
    "labels = labelPaths.map(lambda labelPath: tf.py_function(load_labels, [labelPath], [tf.string, tf.float32]))\n",
    "dataset = tf.data.Dataset.zip(images, labels)\n",
    "\n",
    "dataset = dataset.shuffle(int(len(dataset) / 8)) # shuffle the data a bit (1/8-perfect shuffling)\n",
    "\n",
    "# TODO: poniżej należy wyliczyć rozmiar datasetu testowego tak, by był w stosunku do rozmiaru datasetu treningowego jak 2:8\n",
    "# podpowiedź: należy skorzystać z tego, że tf.data.Dataset implementuje metodę __len__(), zatem można na nim użyć len(...)\n",
    "TEST_SIZE: int = int(...)\n",
    "\n",
    "test_dataset: tf.data.Dataset = dataset.take(TEST_SIZE) \n",
    "train_dataset: tf.data.Dataset = dataset.skip(TEST_SIZE)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} dataset samples\")\n",
    "\n",
    "print()\n",
    "print(\"1st sample:\")\n",
    "samplesIter = iter(dataset)\n",
    "sample = next(samplesIter)\n",
    "(sampleImage, sampleLabel) = sample\n",
    "print(\"Sample type:\", type(sample))\n",
    "print(\"Sample image shape:\", sampleImage.numpy().shape)\n",
    "print(\"Sample label:\")\n",
    "print(sampleLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.save(\"../data/localization-test-dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizualizacja danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def drawBB(image: np.ndarray, text: str, color: Tuple[int, int, int], BB: Tuple[int, int, int, int], scale: float):\n",
    "    x, y, w, h = BB\n",
    "\n",
    "    cv2.rectangle(\n",
    "        image,\n",
    "        # TODO: poniżej:\n",
    "        # - podać jako pt1 współrzędne lewego górnego rogu BB\n",
    "        # - podać jako pt2 współrzędne prawego dolnego rogu BB\n",
    "        pt1 = ...,\n",
    "        pt2 = ...,\n",
    "        color=color,\n",
    "        thickness=int(10 * scale)\n",
    "    )\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        text=text,\n",
    "        org=(x, y - 40),\n",
    "        fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "        fontScale=1.3 * scale,\n",
    "        color=(255, 255, 255),\n",
    "        thickness=int(6 * scale)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "PREVIEW_COLS = 3\n",
    "PREVIEW_ROWS = 3\n",
    "FIG_UNIT_SIZE_W, FIG_UNIT_SIZE_H = 4, 2\n",
    "\n",
    "fig, axs = plt.subplots(nrows=PREVIEW_ROWS, ncols=PREVIEW_COLS, figsize=(PREVIEW_COLS * FIG_UNIT_SIZE_W, PREVIEW_ROWS * FIG_UNIT_SIZE_H))\n",
    "print(f\"Displaying {PREVIEW_COLS * PREVIEW_ROWS} random train dataset samples\")\n",
    "fig.tight_layout()\n",
    "\n",
    "# create an iterator to a copy of the dataset unbatched, i.e., iterated not in batches of shape (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3), but (IMAGE_SIZE, IMAGE_SIZE, 3) - single RGB images\n",
    "samplesIter = iter(train_dataset)\n",
    "\n",
    "for i in range(PREVIEW_ROWS):\n",
    "    for j in range(PREVIEW_COLS):\n",
    "        image, (filename, BBs) = next(samplesIter)\n",
    "        image = image.numpy().reshape(1, IMG_H, IMG_W, 3)[0] # tf.Tensor -> np.ndarray, np.ndarray as uint8 ([0, 255])\n",
    "\n",
    "        for bbi, BB in enumerate(BBs):\n",
    "            (yMin, xMin, yMax, xMax) = BB\n",
    "            \n",
    "            # TODO: poniżej przeliczyć współrzędne wyrażone w wartości znormalizowanej (yMin, xMin, yMax, xMax) na bezwzględne współrzędne pikselowe\n",
    "            xMin = int(...)\n",
    "            xMax = int(...)\n",
    "            yMin = int(...)\n",
    "            yMax = int(...)\n",
    "\n",
    "            drawBB(\n",
    "                image,\n",
    "                text=f\"\",\n",
    "                color=(0, 1, 0),\n",
    "                BB=(\n",
    "                    # TODO: podać poniżej - kolejno - wartości:\n",
    "                    # - X lewego górnego rogu\n",
    "                    # - Y lewego górnego rogu\n",
    "                    # - szerokości BB\n",
    "                    # - wysokości BB\n",
    "                    ...,\n",
    "                    ...,\n",
    "                    ...,\n",
    "                    ...,\n",
    "                ),\n",
    "                scale=0.5\n",
    "            )\n",
    "\n",
    "        axs[i][j].imshow(image, vmin=0, vmax=1)\n",
    "        axs[i][j].set_title(f\"Sample {(i + 1) * PREVIEW_ROWS + j} (detections: {len(BBs)})\")\n",
    "        axs[i][j].axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konwersja przykładów do `tf.train.Example`s i zapis do plików `.tfrecord`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "signClassAsBytes = \"Sign\".encode('utf8')\n",
    "jpegStrAsBytes = \"jpg\".encode('utf8')\n",
    "\n",
    "def create_tf_example(BBs, image, filename: str) -> str:\n",
    "    ymins = [BB[0] for BB in BBs]\n",
    "    xmins = [BB[1] for BB in BBs]\n",
    "    ymaxs = [BB[2] for BB in BBs]\n",
    "    xmaxs = [BB[3] for BB in BBs]\n",
    "    classes_text = [signClassAsBytes] * len(BBs)\n",
    "    classes = [1] * len(BBs)\n",
    "    filename = os.path.splitext(filename)[0].encode(\"utf-8\")\n",
    "\n",
    "    image = (image * 255.0).astype(np.uint8)\n",
    "    imageEncoded = tf.image.encode_jpeg(image, format='rgb').numpy()\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': tf.train.Feature(int64_list=tf.train.Int64List(value=[IMG_H])),\n",
    "        'image/width': tf.train.Feature(int64_list=tf.train.Int64List(value=[IMG_W])),\n",
    "        'image/filename': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/source_id': tf.train.Feature(bytes_list=tf.train.BytesList(value=[filename])),\n",
    "        'image/encoded': tf.train.Feature(bytes_list=tf.train.BytesList(value=[imageEncoded])),\n",
    "        'image/format': tf.train.Feature(bytes_list=tf.train.BytesList(value=[jpegStrAsBytes])),\n",
    "        'image/object/bbox/xmin': tf.train.Feature(float_list=tf.train.FloatList(value=xmins)),\n",
    "        'image/object/bbox/xmax': tf.train.Feature(float_list=tf.train.FloatList(value=xmaxs)),\n",
    "        'image/object/bbox/ymin': tf.train.Feature(float_list=tf.train.FloatList(value=ymins)),\n",
    "        'image/object/bbox/ymax': tf.train.Feature(float_list=tf.train.FloatList(value=ymaxs)),\n",
    "        'image/object/class/text': tf.train.Feature(bytes_list=tf.train.BytesList(value=classes_text)),\n",
    "        'image/object/class/label': tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "    }))\n",
    "\n",
    "    return tf_example.SerializeToString()\n",
    "\n",
    "for ds, datasetLabel in [\n",
    "   (train_dataset, \"train\"),\n",
    "   (test_dataset, \"eval\")\n",
    "]:\n",
    "    outName = f\"signs-{datasetLabel}.tfrecord\"\n",
    "    writer = tf.io.TFRecordWriter(outName)\n",
    "\n",
    "    print(f\"Writing {len(ds)} samples to {outName} (dataset '{datasetLabel}')\")\n",
    "    \n",
    "    for image, (filename, label) in ds:\n",
    "        filename = filename.numpy()\n",
    "\n",
    "        writer.write(\n",
    "            create_tf_example(\n",
    "                BBs=label,\n",
    "                image=image.numpy(),\n",
    "                filename=filename.decode()\n",
    "            )\n",
    "        )\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning sieci\n",
    "\n",
    "#### Konfiguracja zadania trenowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Należy najpierw skonfigurować zadanie trenowania TFM (TF Models API), które spowoduje pobranie i rozpakowanie modelu, który będzie fine-tune'owany (tzn. warstwa wyjściowa będzie trenowana ponownie).\n",
    "\n",
    "Ponadto, po każdych 100 epokach trenowania, checkpoint modelu będzie zapisywany na dysku i będzie uruchamiana ewaluacja, która wyliczy wartości metryk [według specyfikacji ewaluacji datasetu COCO](https://cocodataset.org/#detection-eval) (**z których opisami należy się zapoznać!**) dla modelu w stanie na danym etapie trenowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_classes = 1 # tylko 1 klasa: znak drogowy\n",
    "\n",
    "train_data_input_path = './signs-train.tfrecord'\n",
    "valid_data_input_path = './signs-eval.tfrecord'\n",
    "test_data_input_path = './signs-eval.tfrecord'\n",
    "model_dir = './trained_model/'\n",
    "export_dir ='../models/localizer'\n",
    "\n",
    "exp_config = exp_factory.get_exp_config('retinanet_resnetfpn_coco') # fasterrcnn_resnetfpn_coco retinanet_resnetfpn_coco\n",
    "\n",
    "IMG_SIZE = [IMG_H, IMG_W, 3]\n",
    "\n",
    "# Backbone config.\n",
    "exp_config.task.freeze_backbone = True # do not train the backbone - we just fine-tune the last layer\n",
    "exp_config.task.annotation_file = ''\n",
    "\n",
    "# Model config.\n",
    "exp_config.task.model.input_size = IMG_SIZE\n",
    "exp_config.task.model.num_classes = num_classes + 1\n",
    "# exp_config.task.model.detection_generator.tflite_post_processing.max_classes_per_detection = exp_config.task.model.num_classes\n",
    "\n",
    "# Training data config.\n",
    "exp_config.task.train_data.input_path = train_data_input_path\n",
    "exp_config.task.train_data.dtype = 'float32'\n",
    "exp_config.task.train_data.global_batch_size = batch_size\n",
    "exp_config.task.train_data.parser.aug_scale_max = 1.0\n",
    "exp_config.task.train_data.parser.aug_scale_min = 1.0\n",
    "\n",
    "# Validation data config.\n",
    "exp_config.task.validation_data.input_path = valid_data_input_path\n",
    "exp_config.task.validation_data.dtype = 'float32'\n",
    "exp_config.task.validation_data.global_batch_size = batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Następnie konfigurowana jest ilość epok (iteracji trenowania) oraz ilość kroków na epokę. Po każdej epoce przeprowadzana będzie walidacja - sprawdzenie metryk sieci na zbiorze ewaluacyjnym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "train_steps = 7500\n",
    "exp_config.trainer.steps_per_loop = 100 # steps_per_loop = num_of_training_examples // train_batch_size\n",
    "\n",
    "exp_config.trainer.summary_interval = 100\n",
    "exp_config.trainer.checkpoint_interval = 100\n",
    "exp_config.trainer.validation_interval = 100\n",
    "exp_config.trainer.validation_steps =  100 # validation_steps = num_of_validation_examples // eval_batch_size\n",
    "exp_config.trainer.train_steps = train_steps\n",
    "exp_config.trainer.optimizer_config.warmup.linear.warmup_steps = 100\n",
    "exp_config.trainer.optimizer_config.learning_rate.type = 'cosine'\n",
    "exp_config.trainer.optimizer_config.learning_rate.cosine.decay_steps = train_steps\n",
    "exp_config.trainer.optimizer_config.learning_rate.cosine.initial_learning_rate = 0.1\n",
    "exp_config.trainer.optimizer_config.warmup.linear.warmup_learning_rate = 0.05\n",
    "\n",
    "# pprint.PrettyPrinter(indent=4).pprint(exp_config.as_dict())\n",
    "\n",
    "distribution_strategy = tf.distribute.MirroredStrategy()\n",
    "with distribution_strategy.scope():\n",
    "  task = tfm.core.task_factory.get_task(exp_config.task, logging_dir=model_dir)\n",
    "\n",
    "for images, labels in task.build_inputs(exp_config.task.train_data).take(1):\n",
    "  print()\n",
    "  print(f'images.shape: {str(images.shape):16}  images.dtype: {images.dtype!r}')\n",
    "  print(f'labels.keys: {labels.keys()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Konfiguracja mapy etykiet do ID klas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "category_index = {\n",
    "    # TODO: wpisać do dicta category_index klucz 1 o wartości będącej dictem z kluczami:\n",
    "    # - id o wartości 1\n",
    "    # - name o wartości \"Sign\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wizualizacja przykładowych danych z datasetu treningowego wraz z bounding boxami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tf_ex_decoder = TfExampleDecoder()\n",
    "\n",
    "def visualizeBatchOfRecords(recordsRaw, count):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "\n",
    "    for i, serialized_example in enumerate(recordsRaw):\n",
    "        plt.subplot(1, count, i + 1)\n",
    "        loadedTensors = tf_ex_decoder.decode(serialized_example)\n",
    "        groundTruthBoxes = loadedTensors['groundtruth_boxes'].numpy()\n",
    "        image = loadedTensors['image'].numpy()\n",
    "        scores = np.ones(shape=(len(groundTruthBoxes)))\n",
    "\n",
    "        visualization_utils.visualize_boxes_and_labels_on_image_array(\n",
    "            image,\n",
    "            groundTruthBoxes,\n",
    "            loadedTensors['groundtruth_classes'].numpy().astype('int'),\n",
    "            scores,\n",
    "            category_index=category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            max_boxes_to_draw=200,\n",
    "            min_score_thresh=0.5,\n",
    "            agnostic_mode=False,\n",
    "            instance_masks=None,\n",
    "            line_thickness=4)\n",
    "\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Image-{i + 1}')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "visualizeBatchOfRecords(tf.data.TFRecordDataset(exp_config.task.train_data.input_path).take(4).shuffle(buffer_size=2), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uruchomienie fine-tuningu\n",
    "\n",
    "Wpisana wyżej konfiguracja i dane zostaną teraz wykorzystane do uruchomienia procesu fine-tuningu.\n",
    "\n",
    "**Uwaga:** ten proces potrwa ~10 minut (tylko z uwagi na bardzo mocny sprzęt w tej sali - jest to relatywnie \"mało\"). Na bieżąco będzie można przeglądać wykresy metryk modelu aktualizowane podczas kroku eval (ewaluacji) w lokalnie uruchomionym Tensorboard ([localhost:6006](http://localhost:6006)) uruchamianym w komórce poniżej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "!sleep 8\n",
    "%tensorboard --logdir './trained_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model, eval_logs = tfm.core.train_lib.run_experiment(\n",
    "    distribution_strategy=distribution_strategy,\n",
    "    task=task,\n",
    "    mode='train_and_eval',\n",
    "    params=exp_config,\n",
    "    model_dir=model_dir,\n",
    "    run_post_eval=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.retinanet_model.RetinaNetModel object at 0x7f0c5e7bbc50>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.retinanet_model.RetinaNetModel object at 0x7f0c5e7bbc50>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.detection_generator.MultilevelDetectionGenerator object at 0x7f0c5be446d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of TF-Keras layer <official.vision.modeling.layers.detection_generator.MultilevelDetectionGenerator object at 0x7f0c5be446d0>, because it is not built.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/localizer/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/localizer/assets\n"
     ]
    }
   ],
   "source": [
    "# wyeksportowanie grafu inferencji - zoptymalizowanego formatu modelu wraz z wyuczonymi wagami, na którym można efektywnie przeprowadzać inferencję - tj. predykcję dla dowolnych danych wejściowych\n",
    "\n",
    "export_saved_model_lib.export_inference_graph(\n",
    "    input_type='image_tensor',\n",
    "    batch_size=1,\n",
    "    input_image_size=[IMG_H, IMG_W],\n",
    "    params=exp_config,\n",
    "    checkpoint_path=tf.train.latest_checkpoint(model_dir),\n",
    "    export_dir=export_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uruchomienie inferencji na załadowanym zamrożonym modelu sieci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 przykłady z datasetu - wraz z 'ground truth' bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 3\n",
    "\n",
    "evalDataset = tf.data.TFRecordDataset(\n",
    "    './signs-eval.tfrecord').take(\n",
    "        count)\n",
    "visualizeBatchOfRecords(evalDataset, count)\n",
    "imported = tf.saved_model.load(export_dir)\n",
    "modelFunction = imported.signatures['serving_default']\n",
    "\n",
    "IMG_SIZE = (IMG_H, IMG_W)\n",
    "plt.figure(figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 przykłady z datasetu - wraz z bounding boxami pochodzącymi z inferencji na sieci neuronowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "for i, example in enumerate(evalDataset):\n",
    "  plt.subplot(1, 3, i + 1)\n",
    "  loadedTensors = tf_ex_decoder.decode(example)\n",
    "  imagePath = resize_and_crop_image(\n",
    "      loadedTensors['image'],\n",
    "      IMG_SIZE,\n",
    "      padded_size=IMG_SIZE,\n",
    "      aug_scale_min=1.0,\n",
    "      aug_scale_max=1.0\n",
    "  )[0]\n",
    "  imagePath = tf.expand_dims(imagePath, axis=0)\n",
    "  imagePath = tf.cast(imagePath, dtype = tf.uint8)\n",
    "  image_np = imagePath[0].numpy()\n",
    "  result = modelFunction(imagePath)\n",
    "  visualization_utils.visualize_boxes_and_labels_on_image_array(\n",
    "      image_np,\n",
    "      result['detection_boxes'][0].numpy(),\n",
    "      result['detection_classes'][0].numpy().astype(int),\n",
    "      result['detection_scores'][0].numpy(),\n",
    "      category_index=category_index,\n",
    "      use_normalized_coordinates=False,\n",
    "      max_boxes_to_draw=200,\n",
    "      min_score_thresh=..., # TODO: należy ustawić tą wartość na rozsądny próg (znormalizowany - zakres wartości to 0-1)\n",
    "      min_score_thresh=0.6,\n",
    "      agnostic_mode=False,\n",
    "      instance_masks=None,\n",
    "      line_thickness=4)\n",
    "  plt.imshow(image_np)\n",
    "  plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instrukcja\n",
    "\n",
    "Jako podsumowanie laboratorium należy:\n",
    "- zapisać jako plik `img/localizedBBs.png` powyższą grafikę ze zlokalizowanymi znakami na 3 losowych obrazach z ewaluacyjnego zbioru danych\n",
    "- zapisać fragment ekranu Tensorboard z odpowiednim wykresem jako plik `img/tensorboard-AP.png`\n",
    "- zapisać fragment ekranu Tensorboard z odpowiednim wykresem jako plik `img/tensorboard-AP50.png`\n",
    "- zapisać fragment ekranu Tensorboard z odpowiednim wykresem jako plik `img/tensorboard-AP75.png`\n",
    "- zapoznać się z [dokumentacją standardów ewaluacji datasetu COCO](https://cocodataset.org/#detection-eval) i opisać:\n",
    "    - czym są metryki:\n",
    "        - AP\n",
    "        - AP50\n",
    "        - AP75\n",
    "    - jak należy interpretować wyniki AP, AP50 i AP75 w naszym przypadku: należy opisać, jakie wartości metryk zostały obliczone dla naszego modelu oraz co to oznacza w praktyce?\n",
    "\n",
    "### Przykładowa lokalizacja\n",
    "\n",
    "<img src=\"img/localizedBBs.png?q=1\" />\n",
    "\n",
    "Komentarz dot. skuteczności: ...\n",
    "\n",
    "### Metryki\n",
    "\n",
    "| Metryka | Wartość | Opis metryki | Interpretacja uzyskanej wartości |       Screenshot wykresu z Tensorboard       |\n",
    "|---------|---------|--------------|----------------------------------|----------------------------------------------|\n",
    "| AP      |         |              |                                  |  <img src=\"img/tensorboard-AP.png?q=1\" />    |\n",
    "| AP50    |         |              |                                  |  <img src=\"img/tensorboard-AP50.png?q=1\" />  |\n",
    "| AP75    |         |              |                                  |  <img src=\"img/tensorboard-AP75.png?q=1\" />  |\n",
    "\n",
    "### Interpretacja wyników\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
